{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Define a function to generate the synthetic data considering time zones, weekends, and shutdown\n",
    "def generate_synthetic_data(num_instances, sequence_length=720):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # Timezone to active hours mapping\n",
    "    timezones = {\n",
    "        'UTC': (6, 10),  # Active between 6 AM and 10 AM UTC\n",
    "        'PST': (9, 19),  # Active between 9 AM and 7 PM PST\n",
    "        'CET': (7, 17),  # Active between 7 AM and 5 PM CET\n",
    "        'IND': (0, 12),  # Active between 0 AM and 12 PM CET\n",
    "    }\n",
    "\n",
    "    for _ in range(num_instances):\n",
    "        # Randomly pick a time zone for this server\n",
    "        timezone = random.choice(list(timezones.keys()))\n",
    "        active_start, active_end = timezones[timezone]\n",
    "\n",
    "        # Randomly choose behavior: proper, improper, mostly_off, weekdays_on_weekends_off, or improper_with_maintenance\n",
    "        case_type = random.choice([\"improper\", \"improper\", \"proper\", \"improper\", \"improper\", \"improper\", \"improper\", \"improper\", \"improper\", \"mostly_off\", \"improper\", \"improper_with_maintenance\", \"improper\", \"weekdays_on_weekends_off\", \"improper_with_maintenance\"])\n",
    "\n",
    "        # Initialize sequence and label variables before entering the conditional block\n",
    "        sequence = []\n",
    "        label = None  # Start with None to make sure label is always defined\n",
    "\n",
    "        if case_type == \"proper\":\n",
    "            # Case: Server is ON Monday to Friday, OFF on Saturday and Sunday\n",
    "            for hour in range(sequence_length):\n",
    "                # Determine which day of the week it is (0 = Monday, 6 = Sunday)\n",
    "                day_of_week = (hour // 24) % 7  # 0 to 6 (Mon-Sun)\n",
    "\n",
    "                if day_of_week < 5:  # Weekdays: ON (Mon-Fri)\n",
    "                    # Convert the hour to the correct time zone (local time)\n",
    "                    local_hour = (hour + active_start) % 24  # Adjust for timezone shift\n",
    "\n",
    "                    if local_hour >= active_start and local_hour < active_end:  # ON during business hours\n",
    "                        sequence.append(1)\n",
    "                    else:  # OFF outside of business hours\n",
    "                        sequence.append(0)\n",
    "                else:  # Weekends: OFF (Sat-Sun)\n",
    "                    sequence.append(0)\n",
    "            label = 1  # Proper pause behavior (ON weekdays, OFF weekends)\n",
    "\n",
    "        elif case_type == \"improper\":\n",
    "            # Case 3: Prolonged idle periods (server stays ON for entire period)\n",
    "            sequence = [1] * sequence_length  # Server stays ON for the entire period\n",
    "            label = 0  # Improper pause behavior\n",
    "\n",
    "        elif case_type == \"mostly_off\":\n",
    "            # Case 4: Server is mostly OFF except for patching/maintenance days\n",
    "            sequence = [0] * sequence_length  # Start with all OFF\n",
    "\n",
    "            # Randomly select days for patching (ON)\n",
    "            patch_days = random.sample(range(0, sequence_length // 24), 3)  # Random 3 days for patching\n",
    "            for day in patch_days:\n",
    "                start_hour = day * 24\n",
    "                for hour in range(start_hour, start_hour + 24):  # Mark the full day as ON\n",
    "                    sequence[hour] = 1\n",
    "\n",
    "            label = 1  # Proper behavior but with occasional ON for patching\n",
    "\n",
    "        elif case_type == \"weekdays_on_weekends_off\":\n",
    "            # Case 5: Server is ON during weekdays (Mon-Fri) and OFF on weekends (Sat-Sun)\n",
    "            for hour in range(sequence_length):\n",
    "                # Determine which day of the week it is (0 = Monday, 6 = Sunday)\n",
    "                day_of_week = (hour // 24) % 7  # 0 to 6 (Mon-Sun)\n",
    "\n",
    "                if day_of_week < 5:  # Weekdays: ON (Mon-Fri)\n",
    "                    sequence.append(1)\n",
    "                else:  # Weekends: OFF (Sat-Sun)\n",
    "                    sequence.append(0)\n",
    "            label = 1  # Proper behavior for ON during weekdays and OFF during weekends\n",
    "\n",
    "        elif case_type == \"improper_with_maintenance\":\n",
    "            # Case 6: Server is ON most of the time, but occasionally OFF for patching/maintenance\n",
    "            sequence = [1] * sequence_length  # Server stays ON for most of the time\n",
    "            \n",
    "            # Randomly choose 1-2 days per month (approximately every 30 days) to be OFF for maintenance\n",
    "            maintenance_days = random.sample(range(0, sequence_length // 24), 2)  # Random 1-2 days for maintenance\n",
    "\n",
    "            for day in maintenance_days:\n",
    "                # Randomly pick hours within the day to simulate the server being turned OFF for patching\n",
    "                patch_hours = random.sample(range(day * 24, (day + 1) * 24), 4)  # 4 random hours OFF for patching\n",
    "                for hour in patch_hours:\n",
    "                    sequence[hour] = 0\n",
    "\n",
    "            label = 0  # Improper behavior (server should be off more regularly but is occasionally turned off for patching)\n",
    "\n",
    "        # Append the generated sequence and label to data and labels\n",
    "        data.append(sequence)\n",
    "        labels.append(label)\n",
    "\n",
    "    data = torch.tensor(data).float().unsqueeze(-1)  # Shape (num_instances, sequence_length, 1)\n",
    "    labels = torch.tensor(labels).float().unsqueeze(-1)  # Shape (num_instances, 1)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Example dataset class\n",
    "class EC2Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (torch.Tensor): Shape (num_instances, 720, 1), binary ON/OFF states.\n",
    "            labels (torch.Tensor): Shape (num_instances, 1), binary classification labels.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Generate synthetic data\n",
    "data, labels = generate_synthetic_data(num_instances=10000)\n",
    "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "num_positive = (labels == 1).sum().item()\n",
    "num_negative = (labels == 0).sum().item()\n",
    "print(f\"Positive: {num_positive}, Negative: {num_negative}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = EC2Dataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(64, 32)           # Fully connected layer\n",
    "        self.fc3 = nn.Linear(32, 1)           # Output layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)  # hn is the hidden state of the last LSTM cell\n",
    "        x = self.relu(self.fc1(hn.squeeze(0)))  # Squeeze to remove unnecessary dimension\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create stacked RNN and calibrator for temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemperatureScaling, self).__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1))  # Initialize temperature at 1.0\n",
    "\n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "    \n",
    "class StackedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n",
    "        super(StackedLSTMModel, self).__init__()\n",
    "        # Stacked LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.temperature_scaling = TemperatureScaling()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the stacked LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x)  # hn is the hidden state for the last time step of each layer\n",
    "        \n",
    "        # Use the hidden state from the last LSTM layer (topmost layer)\n",
    "        x = hn[-1]  # hn shape: [num_layers, batch_size, hidden_size], selecting topmost layer's hidden state\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#model = LSTMModel(input_size=1, hidden_size=64)\n",
    "model = StackedLSTMModel(input_size=1, hidden_size=64, num_layers=2)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "LOSS_THRESHOLD = 0.01  # Stop training if loss is below this value\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader.):\n",
    "        # Move data to GPU if available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch_loss < LOSS_THRESHOLD:\n",
    "        print(f\"Stopping early at epoch {epoch+1} as loss < {LOSS_THRESHOLD:.4f}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator = TemperatureScaling()\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizer for temperature scaling\n",
    "optimizer = torch.optim.LBFGS(calibrator.parameters(), lr=0.0001)  # L-BFGS is commonly used for calibration\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use logits for calibration\n",
    "\n",
    "# Training loop\n",
    "model.eval()\n",
    "calibrator.train()\n",
    "calibrator = calibrator.to(device)\n",
    "\n",
    "for epoch in range(500):  # 500 epochs or until convergence\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)  # Get logits from validation data\n",
    "        calibrated_logits = calibrator(logits)\n",
    "        loss = criterion(calibrated_logits, labels)  # Compute loss with true labels\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move data to GPU if available\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on GPU\n",
    "evaluate_model(model, dataloader)\n",
    "# Forward pass without sigmoid\n",
    "raw_outputs = model(inputs)\n",
    "print(f\"Raw Outputs: {raw_outputs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/mnt/data_lake/models/stop_start_trained_model.pth\")\n",
    "#torch.save(calibrator.state_dict(), \"/mnt/data_lake/models/stop_start_trained_model_temperature_scaling.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Define a function to generate one element per case type\n",
    "def generate_single_instance_per_case(sequence_length=720):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    timezones = {\n",
    "        'UTC': (9, 12),\n",
    "        'PST': (0, 3),\n",
    "        'CET': (5, 20),\n",
    "    }\n",
    "\n",
    "    case_types = [\n",
    "        \"proper\",\n",
    "        \"improper\",\n",
    "        \"mostly_off\",\n",
    "        \"weekdays_on_weekends_off\",\n",
    "        \"improper_with_maintenance\",\n",
    "    ]\n",
    "\n",
    "    for case_type in case_types:\n",
    "        # Randomly pick a timezone for this server\n",
    "        timezone = random.choice(list(timezones.keys()))\n",
    "        active_start, active_end = timezones[timezone]\n",
    "\n",
    "        sequence = []\n",
    "        label = None\n",
    "\n",
    "        if case_type == \"proper\":\n",
    "            # ON weekdays, OFF weekends\n",
    "            for hour in range(sequence_length):\n",
    "                day_of_week = (hour // 24) % 7\n",
    "                if day_of_week < 5:  # Weekdays\n",
    "                    local_hour = (hour + active_start) % 24\n",
    "                    sequence.append(1 if active_start <= local_hour < active_end else 0)\n",
    "                else:  # Weekends\n",
    "                    sequence.append(0)\n",
    "            label = 1\n",
    "\n",
    "        elif case_type == \"improper\":\n",
    "            # Always ON\n",
    "            sequence = [1] * sequence_length\n",
    "            label = 0\n",
    "\n",
    "        elif case_type == \"mostly_off\":\n",
    "            # Mostly OFF, some patching days\n",
    "            sequence = [0] * sequence_length\n",
    "            patch_days = random.sample(range(0, sequence_length // 24), 3)\n",
    "            for day in patch_days:\n",
    "                start_hour = day * 24\n",
    "                for hour in range(start_hour, start_hour + 24):\n",
    "                    sequence[hour] = 1\n",
    "            label = 1\n",
    "\n",
    "        elif case_type == \"weekdays_on_weekends_off\":\n",
    "            # ON weekdays, OFF weekends\n",
    "            for hour in range(sequence_length):\n",
    "                day_of_week = (hour // 24) % 7\n",
    "                sequence.append(1 if day_of_week < 5 else 0)\n",
    "            label = 1\n",
    "\n",
    "        elif case_type == \"improper_with_maintenance\":\n",
    "            # Mostly ON, occasional OFF for patching\n",
    "            sequence = [1] * sequence_length\n",
    "            maintenance_days = random.sample(range(0, sequence_length // 24), 2)\n",
    "            for day in maintenance_days:\n",
    "                patch_hours = random.sample(range(day * 24, (day + 1) * 24), 4)\n",
    "                for hour in patch_hours:\n",
    "                    sequence[hour] = 0\n",
    "            label = 0\n",
    "\n",
    "        data.append(sequence)\n",
    "        labels.append(label)\n",
    "\n",
    "    data = torch.tensor(data).float().unsqueeze(-1)\n",
    "    labels = torch.tensor(labels).float().unsqueeze(-1)\n",
    "    return data, labels, case_types\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#model = LSTMModel()\n",
    "model_pred = StackedLSTMModel(input_size=1, hidden_size=64, num_layers=2)\n",
    "#calibrator_eval = TemperatureScaling()\n",
    "model_pred.load_state_dict(torch.load(\"/mnt/data_lake/models/stop_start_trained_model.pth\"))\n",
    "model_pred = model_pred.to(device)\n",
    "model_pred.eval()\n",
    "#calibrator_eval.load_state_dict(torch.load(\"/mnt/data_lake/models/stop_start_trained_model_temperature_scaling.pth\"))\n",
    "#calibrator_eval = calibrator_eval.to(device)\n",
    "#calibrator_eval.eval()\n",
    "\n",
    "\n",
    "# Temperature scaling\n",
    "#logits = model_pred(inputs)  # Get raw logits from the model\n",
    "#calibrated_logits = calibrator(logits)  # Apply temperature scaling\n",
    "#probabilities = torch.sigmoid(calibrated_logits)  # Get calibrated probabilities\n",
    "\n",
    "# Generate synthetic data\n",
    "data, labels, case_types = generate_single_instance_per_case()\n",
    "print(f\"Generated Data Shape: {data.shape}, Labels Shape: {labels.shape}\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    data = data.to(device)\n",
    "    predictions = model_pred(data)  # Pass data through the model\n",
    "    predictions_binary = (predictions > 0.5).float()  # Threshold for binary classification\n",
    "    #logits = model_pred(data)  # Get raw logits\n",
    "    #calibrated_logits = calibrator(logits)\n",
    "    #probabilities = torch.sigmoid(calibrated_logits)  # Final probabilities\n",
    "\n",
    "# Print results\n",
    "for i, case in enumerate(case_types):\n",
    "    print(f\"Case: {case}\")\n",
    "    print(f\"  True Label: {int(labels[i].item())}\")\n",
    "    print(f\"  Predicted Label: {int(predictions_binary[i].item())}\")\n",
    "    print(f\"  Confidence: {predictions[i].item():.2f}\")\n",
    "    #print(f\"  Calibrated: {calibrated_logits[i].item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "data, labels = generate_synthetic_data(num_instances=1000)\n",
    "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "num_positive = (labels == 1).sum().item()\n",
    "num_negative = (labels == 0).sum().item()\n",
    "print(f\"Positive: {num_positive}, Negative: {num_negative}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = data.to(device)\n",
    "    predictions = model_pred(data)  # Pass data through the model\n",
    "    predictions_binary = (predictions > 0.5).float()  # Threshold for binary classification\n",
    "\n",
    "pre_num_positive = (predictions_binary == 1).sum().item()\n",
    "pre_num_negative = (predictions_binary == 0).sum().item()\n",
    "print(f\"Predicted Positive: {pre_num_positive}, Predicted Negative: {pre_num_negative}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
