{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Define a function to generate the synthetic data considering time zones, weekends, and shutdown\n",
    "def generate_synthetic_data(num_instances, sequence_length=720):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # Timezone to active hours mapping\n",
    "    timezones = {\n",
    "        'UTC': (6, 10),  # Active between 6 AM and 10 AM UTC\n",
    "        'PST': (9, 19),  # Active between 9 AM and 7 PM PST\n",
    "        'CET': (7, 17),  # Active between 7 AM and 5 PM CET\n",
    "        'IND': (0, 12),  # Active between 0 AM and 12 PM CET\n",
    "    }\n",
    "\n",
    "    for _ in range(num_instances):\n",
    "        # Randomly pick a time zone for this server\n",
    "        timezone = random.choice(list(timezones.keys()))\n",
    "        active_start, active_end = timezones[timezone]\n",
    "\n",
    "        # Randomly choose behavior: proper, improper, mostly_off, weekdays_on_weekends_off, or improper_with_maintenance\n",
    "        case_type = random.choice([\"improper\", \"improper\", \"proper\", \"improper\", \"improper\", \"improper\", \"improper\", \"improper\", \"improper\", \"mostly_off\", \"improper\", \"improper_with_maintenance\", \"improper\", \"weekdays_on_weekends_off\", \"improper_with_maintenance\"])\n",
    "\n",
    "        # Initialize sequence and label variables before entering the conditional block\n",
    "        sequence = []\n",
    "        label = None  # Start with None to make sure label is always defined\n",
    "\n",
    "        if case_type == \"proper\":\n",
    "            # Case: Server is ON Monday to Friday, OFF on Saturday and Sunday\n",
    "            for hour in range(sequence_length):\n",
    "                # Determine which day of the week it is (0 = Monday, 6 = Sunday)\n",
    "                day_of_week = (hour // 24) % 7  # 0 to 6 (Mon-Sun)\n",
    "\n",
    "                if day_of_week < 5:  # Weekdays: ON (Mon-Fri)\n",
    "                    # Convert the hour to the correct time zone (local time)\n",
    "                    local_hour = (hour + active_start) % 24  # Adjust for timezone shift\n",
    "\n",
    "                    if local_hour >= active_start and local_hour < active_end:  # ON during business hours\n",
    "                        sequence.append(1)\n",
    "                    else:  # OFF outside of business hours\n",
    "                        sequence.append(0)\n",
    "                else:  # Weekends: OFF (Sat-Sun)\n",
    "                    sequence.append(0)\n",
    "            label = 1  # Proper pause behavior (ON weekdays, OFF weekends)\n",
    "\n",
    "        elif case_type == \"improper\":\n",
    "            # Case 3: Prolonged idle periods (server stays ON for entire period)\n",
    "            sequence = [1] * sequence_length  # Server stays ON for the entire period\n",
    "            label = 0  # Improper pause behavior\n",
    "\n",
    "        elif case_type == \"mostly_off\":\n",
    "            # Case 4: Server is mostly OFF except for patching/maintenance days\n",
    "            sequence = [0] * sequence_length  # Start with all OFF\n",
    "\n",
    "            # Randomly select days for patching (ON)\n",
    "            patch_days = random.sample(range(0, sequence_length // 24), 3)  # Random 3 days for patching\n",
    "            for day in patch_days:\n",
    "                start_hour = day * 24\n",
    "                for hour in range(start_hour, start_hour + 24):  # Mark the full day as ON\n",
    "                    sequence[hour] = 1\n",
    "\n",
    "            label = 1  # Proper behavior but with occasional ON for patching\n",
    "\n",
    "        elif case_type == \"weekdays_on_weekends_off\":\n",
    "            # Case 5: Server is ON during weekdays (Mon-Fri) and OFF on weekends (Sat-Sun)\n",
    "            for hour in range(sequence_length):\n",
    "                # Determine which day of the week it is (0 = Monday, 6 = Sunday)\n",
    "                day_of_week = (hour // 24) % 7  # 0 to 6 (Mon-Sun)\n",
    "\n",
    "                if day_of_week < 5:  # Weekdays: ON (Mon-Fri)\n",
    "                    sequence.append(1)\n",
    "                else:  # Weekends: OFF (Sat-Sun)\n",
    "                    sequence.append(0)\n",
    "            label = 1  # Proper behavior for ON during weekdays and OFF during weekends\n",
    "\n",
    "        elif case_type == \"improper_with_maintenance\":\n",
    "            # Case 6: Server is ON most of the time, but occasionally OFF for patching/maintenance\n",
    "            sequence = [1] * sequence_length  # Server stays ON for most of the time\n",
    "            \n",
    "            # Randomly choose 1-2 days per month (approximately every 30 days) to be OFF for maintenance\n",
    "            maintenance_days = random.sample(range(0, sequence_length // 24), 2)  # Random 1-2 days for maintenance\n",
    "\n",
    "            for day in maintenance_days:\n",
    "                # Randomly pick hours within the day to simulate the server being turned OFF for patching\n",
    "                patch_hours = random.sample(range(day * 24, (day + 1) * 24), 4)  # 4 random hours OFF for patching\n",
    "                for hour in patch_hours:\n",
    "                    sequence[hour] = 0\n",
    "\n",
    "            label = 0  # Improper behavior (server should be off more regularly but is occasionally turned off for patching)\n",
    "\n",
    "        # Append the generated sequence and label to data and labels\n",
    "        data.append(sequence)\n",
    "        labels.append(label)\n",
    "\n",
    "    data = torch.tensor(data).float().unsqueeze(-1)  # Shape (num_instances, sequence_length, 1)\n",
    "    labels = torch.tensor(labels).float().unsqueeze(-1)  # Shape (num_instances, 1)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([10000, 720, 1]), Labels shape: torch.Size([10000, 1])\n",
      "Positive: 1993, Negative: 8007\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Example dataset class\n",
    "class EC2Dataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (torch.Tensor): Shape (num_instances, 720, 1), binary ON/OFF states.\n",
    "            labels (torch.Tensor): Shape (num_instances, 1), binary classification labels.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Generate synthetic data\n",
    "data, labels = generate_synthetic_data(num_instances=10000)\n",
    "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "num_positive = (labels == 1).sum().item()\n",
    "num_negative = (labels == 0).sum().item()\n",
    "print(f\"Positive: {num_positive}, Negative: {num_negative}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = EC2Dataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(64, 32)           # Fully connected layer\n",
    "        self.fc3 = nn.Linear(32, 1)           # Output layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)  # hn is the hidden state of the last LSTM cell\n",
    "        x = self.relu(self.fc1(hn.squeeze(0)))  # Squeeze to remove unnecessary dimension\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create stacked RNN and calibrator for temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemperatureScaling, self).__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1))  # Initialize temperature at 1.0\n",
    "\n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StackedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(StackedLSTMModel, self).__init__()\n",
    "        # Stacked LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, int(hidden_size / 2))\n",
    "        self.fc2 = nn.Linear(int(hidden_size / 2), int(hidden_size / 4))\n",
    "        self.fc3 = nn.Linear(int(hidden_size / 4), output_size)\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  # Use only if task requires probabilities\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the stacked LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # Use the hidden state from the last LSTM layer\n",
    "        x = hn[-1]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Final output\n",
    "        \n",
    "        # Apply sigmoid only if probabilities are needed\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StackedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, rnn_type='RNN', bidirectional=False, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the StackedRNN model.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size (int): Size of the input features.\n",
    "        - hidden_size (int): Number of hidden units in each RNN layer.\n",
    "        - num_layers (int): Number of stacked RNN layers.\n",
    "        - rnn_type (str): Type of RNN ('RNN', 'LSTM', 'GRU').\n",
    "        - bidirectional (bool): If True, use a bidirectional RNN.\n",
    "        - dropout (float): Dropout rate applied between layers (ignored if num_layers < 2).\n",
    "        \"\"\"\n",
    "        super(StackedRNN, self).__init__()\n",
    "        self.rnn_type = rnn_type.upper()\n",
    "        \n",
    "        # Select RNN type\n",
    "        if self.rnn_type == 'RNN':\n",
    "            rnn_layer = nn.RNN\n",
    "        elif self.rnn_type == 'LSTM':\n",
    "            rnn_layer = nn.LSTM\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            rnn_layer = nn.GRU\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported rnn_type. Choose from 'RNN', 'LSTM', 'GRU'.\")\n",
    "\n",
    "        # RNN model\n",
    "        self.rnn = rnn_layer(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the RNN.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "        - hidden (Tensor or tuple): Initial hidden state or hidden+cell states (for LSTM).\n",
    "\n",
    "        Returns:\n",
    "        - output (Tensor): Output tensor of shape (batch_size, sequence_length, hidden_size * num_directions).\n",
    "        - hidden (Tensor or tuple): Final hidden state or hidden+cell states.\n",
    "        \"\"\"\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/5, Loss: 0.1202\n",
      "Epoch 2/5, Loss: 0.0001\n",
      "Epoch 3/5, Loss: 0.0000\n",
      "Stopping early at epoch 4 as loss < 0.0100\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#model = LSTMModel(input_size=1, hidden_size=64)\n",
    "model = StackedLSTMModel(input_size=1, hidden_size=180, num_layers=2)\n",
    "#model = StackedRNN(input_size=1, hidden_size=90, num_layers=2, rnn_type='LSTM', bidirectional=True, dropout=0.0)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "LOSS_THRESHOLD = 0.01  # Stop training if loss is below this value\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        # Move data to GPU if available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch_loss < LOSS_THRESHOLD:\n",
    "        print(f\"Stopping early at epoch {epoch+1} as loss < {LOSS_THRESHOLD:.4f}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator = TemperatureScaling()\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optimizer for temperature scaling\n",
    "optimizer = torch.optim.LBFGS(calibrator.parameters(), lr=0.0001)  # L-BFGS is commonly used for calibration\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use logits for calibration\n",
    "\n",
    "# Training loop\n",
    "model.eval()\n",
    "calibrator.train()\n",
    "calibrator = calibrator.to(device)\n",
    "\n",
    "for epoch in range(500):  # 500 epochs or until convergence\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)  # Get logits from validation data\n",
    "        calibrated_logits = calibrator(logits)\n",
    "        loss = criterion(calibrated_logits, labels)  # Compute loss with true labels\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Raw Outputs: tensor([[1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [9.9999e-01],\n",
      "        [1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [9.9999e-01],\n",
      "        [1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [1.1410e-05],\n",
      "        [9.9999e-01]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move data to GPU if available\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on GPU\n",
    "evaluate_model(model, dataloader)\n",
    "# Forward pass without sigmoid\n",
    "raw_outputs = model(inputs)\n",
    "print(f\"Raw Outputs: {raw_outputs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/mnt/data_lake/models/stop_start_trained_model.pth\")\n",
    "#torch.save(calibrator.state_dict(), \"/mnt/data_lake/models/stop_start_trained_model_temperature_scaling.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Define a function to generate one element per case type\n",
    "def generate_single_instance_per_case(sequence_length=720):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    timezones = {\n",
    "        'UTC': (9, 12),\n",
    "        'PST': (0, 3),\n",
    "        'CET': (5, 20),\n",
    "    }\n",
    "\n",
    "    case_types = [\n",
    "        \"proper\",\n",
    "        \"improper\",\n",
    "        \"mostly_off\",\n",
    "        \"weekdays_on_weekends_off\",\n",
    "        \"improper_with_maintenance\",\n",
    "    ]\n",
    "\n",
    "    for case_type in case_types:\n",
    "        # Randomly pick a timezone for this server\n",
    "        timezone = random.choice(list(timezones.keys()))\n",
    "        active_start, active_end = timezones[timezone]\n",
    "\n",
    "        sequence = []\n",
    "        label = None\n",
    "\n",
    "        if case_type == \"proper\":\n",
    "            # ON weekdays, OFF weekends\n",
    "            for hour in range(sequence_length):\n",
    "                day_of_week = (hour // 24) % 7\n",
    "                if day_of_week < 5:  # Weekdays\n",
    "                    local_hour = (hour + active_start) % 24\n",
    "                    sequence.append(1 if active_start <= local_hour < active_end else 0)\n",
    "                else:  # Weekends\n",
    "                    sequence.append(0)\n",
    "            label = 1\n",
    "\n",
    "        elif case_type == \"improper\":\n",
    "            # Always ON\n",
    "            sequence = [1] * sequence_length\n",
    "            label = 0\n",
    "\n",
    "        elif case_type == \"mostly_off\":\n",
    "            # Mostly OFF, some patching days\n",
    "            sequence = [0] * sequence_length\n",
    "            patch_days = random.sample(range(0, sequence_length // 24), 3)\n",
    "            for day in patch_days:\n",
    "                start_hour = day * 24\n",
    "                for hour in range(start_hour, start_hour + 24):\n",
    "                    sequence[hour] = 1\n",
    "            label = 1\n",
    "\n",
    "        elif case_type == \"weekdays_on_weekends_off\":\n",
    "            # ON weekdays, OFF weekends\n",
    "            for hour in range(sequence_length):\n",
    "                day_of_week = (hour // 24) % 7\n",
    "                sequence.append(1 if day_of_week < 5 else 0)\n",
    "            label = 1\n",
    "\n",
    "        elif case_type == \"improper_with_maintenance\":\n",
    "            # Mostly ON, occasional OFF for patching\n",
    "            sequence = [1] * sequence_length\n",
    "            maintenance_days = random.sample(range(0, sequence_length // 24), 2)\n",
    "            for day in maintenance_days:\n",
    "                patch_hours = random.sample(range(day * 24, (day + 1) * 24), 4)\n",
    "                for hour in patch_hours:\n",
    "                    sequence[hour] = 0\n",
    "            label = 0\n",
    "\n",
    "        data.append(sequence)\n",
    "        labels.append(label)\n",
    "\n",
    "    data = torch.tensor(data).float().unsqueeze(-1)\n",
    "    labels = torch.tensor(labels).float().unsqueeze(-1)\n",
    "    return data, labels, case_types\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29/711571876.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_pred.load_state_dict(torch.load(\"/mnt/data_lake/models/stop_start_trained_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Data Shape: torch.Size([5, 720, 1]), Labels Shape: torch.Size([5, 1])\n",
      "Case: proper\n",
      "  True Label: 1\n",
      "  Predicted Label: 1\n",
      "  Confidence: 1.00\n",
      "Case: improper\n",
      "  True Label: 0\n",
      "  Predicted Label: 0\n",
      "  Confidence: 0.00\n",
      "Case: mostly_off\n",
      "  True Label: 1\n",
      "  Predicted Label: 1\n",
      "  Confidence: 1.00\n",
      "Case: weekdays_on_weekends_off\n",
      "  True Label: 1\n",
      "  Predicted Label: 1\n",
      "  Confidence: 1.00\n",
      "Case: improper_with_maintenance\n",
      "  True Label: 0\n",
      "  Predicted Label: 0\n",
      "  Confidence: 0.00\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#model = LSTMModel()\n",
    "model_pred = StackedLSTMModel(input_size=1, hidden_size=180, num_layers=2)\n",
    "#calibrator_eval = TemperatureScaling()\n",
    "model_pred.load_state_dict(torch.load(\"/mnt/data_lake/models/stop_start_trained_model.pth\"))\n",
    "model_pred = model_pred.to(device)\n",
    "model_pred.eval()\n",
    "#calibrator_eval.load_state_dict(torch.load(\"/mnt/data_lake/models/stop_start_trained_model_temperature_scaling.pth\"))\n",
    "#calibrator_eval = calibrator_eval.to(device)\n",
    "#calibrator_eval.eval()\n",
    "\n",
    "\n",
    "# Temperature scaling\n",
    "#logits = model_pred(inputs)  # Get raw logits from the model\n",
    "#calibrated_logits = calibrator(logits)  # Apply temperature scaling\n",
    "#probabilities = torch.sigmoid(calibrated_logits)  # Get calibrated probabilities\n",
    "\n",
    "# Generate synthetic data\n",
    "data, labels, case_types = generate_single_instance_per_case()\n",
    "print(f\"Generated Data Shape: {data.shape}, Labels Shape: {labels.shape}\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    data = data.to(device)\n",
    "    predictions = model_pred(data)  # Pass data through the model\n",
    "    predictions_binary = (predictions > 0.5).float()  # Threshold for binary classification\n",
    "    #logits = model_pred(data)  # Get raw logits\n",
    "    #calibrated_logits = calibrator(logits)\n",
    "    #probabilities = torch.sigmoid(calibrated_logits)  # Final probabilities\n",
    "\n",
    "# Print results\n",
    "for i, case in enumerate(case_types):\n",
    "    print(f\"Case: {case}\")\n",
    "    print(f\"  True Label: {int(labels[i].item())}\")\n",
    "    print(f\"  Predicted Label: {int(predictions_binary[i].item())}\")\n",
    "    print(f\"  Confidence: {predictions[i].item():.2f}\")\n",
    "    #print(f\"  Calibrated: {calibrated_logits[i].item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([1000, 720, 1]), Labels shape: torch.Size([1000, 1])\n",
      "Positive: 186, Negative: 814\n",
      "Predicted Positive: 186, Predicted Negative: 814\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "data, labels = generate_synthetic_data(num_instances=1000)\n",
    "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "num_positive = (labels == 1).sum().item()\n",
    "num_negative = (labels == 0).sum().item()\n",
    "print(f\"Positive: {num_positive}, Negative: {num_negative}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = data.to(device)\n",
    "    predictions = model_pred(data)  # Pass data through the model\n",
    "    predictions_binary = (predictions > 0.5).float()  # Threshold for binary classification\n",
    "\n",
    "pre_num_positive = (predictions_binary == 1).sum().item()\n",
    "pre_num_negative = (predictions_binary == 0).sum().item()\n",
    "print(f\"Predicted Positive: {pre_num_positive}, Predicted Negative: {pre_num_negative}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
